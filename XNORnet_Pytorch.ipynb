{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XNORnet-Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyasaswy/EE595-Project/blob/master/XNORnet_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtVEyMDh0eO3",
        "colab_type": "text"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrIdQE-Yzqft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import sys\n",
        "import numpy\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svD-JfGq0Z4m",
        "colab_type": "text"
      },
      "source": [
        "Binarize the Input Activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af4zCWkZztv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinActive(torch.autograd.Function):\n",
        "    '''\n",
        "    Binarize the input activations and calculate the mean across channel dimension.\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def forward(self, input):\n",
        "        self.save_for_backward(input)\n",
        "        size = input.size()\n",
        "        input = input.sign()\n",
        "        return input\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        input, = self.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[input.ge(1)] = 0\n",
        "        grad_input[input.le(-1)] = 0\n",
        "        return grad_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT-fa19g0Xi6",
        "colab_type": "text"
      },
      "source": [
        "Binary Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UyaDhrCzvz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class convSFQ(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels,\n",
        "            kernel_size=-1, stride=-1, padding=-1, groups=1, dropout=0,\n",
        "            Linear=False, previous_conv=False, size=0):\n",
        "        super(convSFQ, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.layer_type = 'convSFQ'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dropout_ratio = dropout\n",
        "        self.previous_conv = previous_conv\n",
        "        print(input_channels, output_channels, kernel_size, stride, padding, groups)\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels,\n",
        "                    kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "        \n",
        "    def forward(self, x):        \n",
        "        x = self.conv(x)\n",
        "        return(x)\n",
        "\n",
        "class BinConv2d(nn.Module): # change the name of BinConv2d\n",
        "    def __init__(self, input_channels, output_channels,\n",
        "            kernel_size=-1, stride=-1, padding=-1, groups=1, dropout=0,\n",
        "            Linear=False, previous_conv=False, size=0):\n",
        "        super(BinConv2d, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.layer_type = 'BinConv2d'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dropout_ratio = dropout\n",
        "        self.previous_conv = previous_conv\n",
        "\n",
        "\n",
        "    # (bn): BatchNorm2d(20, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    # (conv): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
        "    # (relu): ReLU(inplace=True)\n",
        "\n",
        "\n",
        "        if dropout!=0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        self.Linear = Linear\n",
        "        if not self.Linear:\n",
        "            self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
        "            #self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            #print(input_channels, output_channels, kernel_size, stride, padding, groups)\n",
        "            self.conv = convSFQ(input_channels, output_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "        else:\n",
        "            if self.previous_conv:\n",
        "                self.bn = nn.BatchNorm2d(int(input_channels/size), eps=1e-4, momentum=0.1, affine=True)\n",
        "            else:\n",
        "                self.bn = nn.BatchNorm1d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
        "            self.linear = nn.Linear(input_channels, output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = BinActive.apply(x)\n",
        "        if self.dropout_ratio!=0:\n",
        "            x = self.dropout(x)\n",
        "        if not self.Linear:\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            if self.previous_conv:\n",
        "                x = x.view(x.size(0), self.input_channels)\n",
        "            x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdX_FySX0VIJ",
        "colab_type": "text"
      },
      "source": [
        "LeNet - 5 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTpNad9Yzy_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet_5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet_5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 2, kernel_size=5, stride=1)\n",
        "        self.bn_conv1 = nn.BatchNorm2d(2, eps=1e-4, momentum=0.1, affine=False)\n",
        "        self.relu_conv1 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.bin_conv2 = BinConv2d(2, 2, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.bin_ip1 = BinConv2d(2*4*4, 500, Linear=True,\n",
        "                previous_conv=True, size=4*4)\n",
        "        self.ip2 = nn.Linear(500, 10)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                if hasattr(m.weight, 'data'):\n",
        "                    m.weight.data.zero_().add_(1.0)\n",
        "        return\n",
        "\n",
        "    def forward(self, x):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                if hasattr(m.weight, 'data'):\n",
        "                    m.weight.data.clamp_(min=0.01)\n",
        "        #orch.Size([128, 1, 28, 28])\n",
        "        x = self.conv1(x)           # torch.Size([128, 20, 24, 24])\n",
        "        x = self.bn_conv1(x)        # torch.Size([128, 20, 24, 24])\n",
        "        x = self.relu_conv1(x)      # torch.Size([128, 20, 24, 24])\n",
        "        x = self.pool1(x)           # torch.Size([128, 20, 12, 12])\n",
        "        x = self.bin_conv2(x)       # torch.Size([128, 50, 8, 8])\n",
        "        x = self.pool2(x)           # torch.Size([128, 50, 4, 4])\n",
        "\n",
        "        # x = x.view(x.size(0), 50*4*4)\n",
        "\n",
        "        x = self.bin_ip1(x)         # torch.Size([128, 500])\n",
        "        x = self.ip2(x)             # torch.Size([128, 10])\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMpw3IyH0SOo",
        "colab_type": "text"
      },
      "source": [
        "Binarize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JuPRE6Vzz0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinOp():\n",
        "    def __init__(self, model):\n",
        "        # count the number of Conv2d and Linear\n",
        "        count_targets = 0\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                count_targets = count_targets + 1\n",
        "\n",
        "        start_range = 1\n",
        "        end_range = count_targets-2\n",
        "        self.bin_range = numpy.linspace(start_range,\n",
        "                end_range, end_range-start_range+1)\\\n",
        "                        .astype('int').tolist()\n",
        "        self.num_of_params = len(self.bin_range)\n",
        "        self.saved_params = []\n",
        "        self.target_modules = []\n",
        "        index = -1\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                index = index + 1\n",
        "                if index in self.bin_range:\n",
        "                    tmp = m.weight.data.clone()\n",
        "                    self.saved_params.append(tmp)\n",
        "                    self.target_modules.append(m.weight)\n",
        "        return\n",
        "\n",
        "    def binarization(self):\n",
        "        self.meancenterConvParams()\n",
        "        self.clampConvParams()\n",
        "        self.save_params()\n",
        "        self.binarizeConvParams()\n",
        "\n",
        "    def meancenterConvParams(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            s = self.target_modules[index].data.size()\n",
        "            negMean = self.target_modules[index].data.mean(1, keepdim=True).\\\n",
        "                    mul(-1).expand_as(self.target_modules[index].data)\n",
        "            self.target_modules[index].data = self.target_modules[index].data.add(negMean)\n",
        "        \n",
        "\n",
        "    def clampConvParams(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            self.target_modules[index].data = \\\n",
        "                    self.target_modules[index].data.clamp(-1.0, 1.0)\n",
        "\n",
        "    def save_params(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            self.saved_params[index].copy_(self.target_modules[index].data)\n",
        "\n",
        "    def binarizeConvParams(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            n = self.target_modules[index].data[0].nelement()\n",
        "            s = self.target_modules[index].data.size()\n",
        "            if len(s) == 4:\n",
        "                m = self.target_modules[index].data.norm(1, 3, keepdim=True)\\\n",
        "                        .sum(2, keepdim=True).sum(1, keepdim=True).div(n)\n",
        "            elif len(s) == 2:\n",
        "                m = self.target_modules[index].data.norm(1, 1, keepdim=True).div(n)\n",
        "            self.target_modules[index].data = \\\n",
        "                    self.target_modules[index].data.sign().mul(m.expand(s))\n",
        "\n",
        "    def restore(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            self.target_modules[index].data.copy_(self.saved_params[index])\n",
        "\n",
        "    def updateBinaryGradWeight(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            weight = self.target_modules[index].data\n",
        "            n = weight[0].nelement()\n",
        "            s = weight.size()\n",
        "            if len(s) == 4:\n",
        "                m = weight.norm(1, 3, keepdim=True)\\\n",
        "                        .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n",
        "            elif len(s) == 2:\n",
        "                m = weight.norm(1, 1, keepdim=True).div(n).expand(s)\n",
        "            m[weight.lt(-1.0)] = 0 \n",
        "            m[weight.gt(1.0)] = 0\n",
        "            m = m.mul(self.target_modules[index].grad.data)\n",
        "            m_add = weight.sign().mul(self.target_modules[index].grad.data)\n",
        "            if len(s) == 4:\n",
        "                m_add = m_add.sum(3, keepdim=True)\\\n",
        "                        .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n",
        "            elif len(s) == 2:\n",
        "                m_add = m_add.sum(1, keepdim=True).div(n).expand(s)\n",
        "            m_add = m_add.mul(weight.sign())\n",
        "            self.target_modules[index].grad.data = m.add(m_add).mul(1.0-1.0/s[1]).mul(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYBCD0JA0PY0",
        "colab_type": "text"
      },
      "source": [
        "Train, Test and Adjust Learning Rate Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g23-nMu5z3-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if args.cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # process the weights including binarization\n",
        "        bin_op.binarization()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        # restore weights\n",
        "        bin_op.restore()\n",
        "        bin_op.updateBinaryGradWeight()\n",
        "\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
        "    return\n",
        "\n",
        "def test(evaluate=False):\n",
        "    global best_acc\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    bin_op.binarization()\n",
        "    j = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "    # for data, target in test_loader:\n",
        "        j = j + 1\n",
        "        if  j >0:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data, volatile=True), Variable(target)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).data.item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            # print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            #     epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            #     100. * batch_idx / len(train_loader), test_loss))\n",
        "\n",
        "    bin_op.restore()\n",
        "    \n",
        "    acc = 100. * float(correct) / len(test_loader.dataset)\n",
        "    if (acc > best_acc):\n",
        "        best_acc = acc\n",
        "        # if not evaluate:\n",
        "            # save_state(model, best_acc)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        test_loss * args.batch_size, correct, len(test_loader.dataset),\n",
        "        100. * float(correct) / len(test_loader.dataset)))\n",
        "    print('Best Accuracy: {:.2f}%\\n'.format(best_acc))\n",
        "    return\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 15 epochs\"\"\"\n",
        "    lr = args.lr * (0.1 ** (epoch // args.lr_epochs))\n",
        "    print('Learning rate:', lr)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN8x0QBc0Mfg",
        "colab_type": "text"
      },
      "source": [
        "Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbcCzEsuz6PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BNN_ARGS:\n",
        "  def __init__(self):\n",
        "    self.batch_size = 128\n",
        "    self.test_batch_size = 128\n",
        "    self.epochs = 1\n",
        "    self.lr_epochs = 15\n",
        "    self.lr = 0.01\n",
        "    self.momentum = 0.9\n",
        "    self.weight_decay = 1e-5\n",
        "    self.no_cuda = False\n",
        "    self.seed = 1\n",
        "    self.log_interval = 100\n",
        "    self.arch = 'LeNet_5'\n",
        "    self.pretrained = None\n",
        "    self.evaluate = False\n",
        "    self.cuda = not self.no_cuda and torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50-N6SNv0LRi",
        "colab_type": "text"
      },
      "source": [
        "Main Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzX-Fj0kz8WY",
        "colab_type": "code",
        "outputId": "cade4ccb-72c3-4428-ee80-b01d69dad82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "args = BNN_ARGS()\n",
        "\n",
        "print(args)\n",
        "    \n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "  torch.cuda.manual_seed(args.seed)\n",
        "    \n",
        "    # load data\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('data', train=True, download=True,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))\n",
        "                ])),\n",
        "            batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "            ])),\n",
        "            batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
        "    \n",
        "    # generate the model\n",
        "# test_loader = test_loader[:3000]\n",
        "if args.arch == 'LeNet_5':\n",
        "    model = LeNet_5()\n",
        "else:\n",
        "    print('ERROR: specified arch is not suppported')\n",
        "    exit()\n",
        "\n",
        "if not args.pretrained:\n",
        "    best_acc = 0.0\n",
        "else:\n",
        "    pretrained_model = torch.load(args.pretrained)\n",
        "    best_acc = pretrained_model['acc']\n",
        "    model.load_state_dict(pretrained_model['state_dict'])\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    \n",
        "print(model)\n",
        "param_dict = dict(model.named_parameters())\n",
        "params = []\n",
        "    \n",
        "base_lr = 0.1\n",
        "    \n",
        "for key, value in param_dict.items():\n",
        "    params += [{'params':[value], 'lr': args.lr,\n",
        "        'weight_decay': args.weight_decay,\n",
        "        'key':key}]\n",
        "    \n",
        "optimizer = optim.Adam(params, lr=args.lr,\n",
        "        weight_decay=args.weight_decay)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "bin_op = BinOp(model)\n",
        "\n",
        "if args.evaluate:\n",
        "  test(evaluate=True)\n",
        "  exit()\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.BNN_ARGS object at 0x7f671e237860>\n",
            "2 2 5 1 0 1\n",
            "LeNet_5(\n",
            "  (conv1): Conv2d(1, 2, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn_conv1): BatchNorm2d(2, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
            "  (relu_conv1): ReLU(inplace=True)\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (bin_conv2): BinConv2d(\n",
            "    (bn): BatchNorm2d(2, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv): convSFQ(\n",
            "      (conv): Conv2d(2, 2, kernel_size=(5, 5), stride=(1, 1))\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (bin_ip1): BinConv2d(\n",
            "    (bn): BatchNorm2d(2, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (linear): Linear(in_features=32, out_features=500, bias=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (ip2): Linear(in_features=500, out_features=10, bias=True)\n",
            ")\n",
            "Learning rate: 0.01\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.290885\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.791374\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.412620\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.541486\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.531637\n",
            "\n",
            "Test set: Average loss: 0.5348, Accuracy: 8170/10000 (81.70%)\n",
            "Best Accuracy: 81.70%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GYd9lyn0Oeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class convSFQ_test(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels,\n",
        "            kernel_size=-1, stride=-1, padding=-1, groups=1, dropout=0,\n",
        "            Linear=False, previous_conv=False, size=0):\n",
        "        super(convSFQ_test, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.layer_type = 'convSFQ_test'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dropout_ratio = dropout\n",
        "        self.previous_conv = previous_conv\n",
        "        print(input_channels, output_channels, kernel_size, stride, padding, groups)\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels,\n",
        "                    kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # # print('Executing Custom Convolution')\n",
        "        wt = self.conv.weight.data.numpy()\n",
        "        w_sets = wt.shape[0]\n",
        "        w_fets = wt.shape[1]\n",
        "        w_rows = wt.shape[2]\n",
        "        w_cols = wt.shape[3]\n",
        "\n",
        "        zeroMat = torch.zeros(x.shape)\n",
        "        input_BN = torch.where(x > 0, x, zeroMat)\n",
        "        inputNP = input_BN.data.numpy()\n",
        "        sets = inputNP.shape[0]\n",
        "        fets = inputNP.shape[1]\n",
        "        rows = inputNP.shape[2]\n",
        "        cols = inputNP.shape[3]\n",
        "\n",
        "        output = np.empty((sets, w_sets, rows-w_rows+1, cols-w_cols+1))\n",
        "\n",
        "        # print(' sets = ', sets, ' fets = ', w_sets, ' Rows = ', rows-w_rows+1, ' Cols = ', cols-w_cols+1, ' F = ', w_fets, ' R = ', w_rows)\n",
        "        # Total_Executions = sets*w_sets*(rows-w_rows+1)*(cols-w_cols+1)*w_fets*w_rows\n",
        "        # ETC = (sets*w_sets*(rows-w_rows+1)*(cols-w_cols+1)*w_fets*w_rows)/3600\n",
        "        # print('Estimated Time = ', ETC, ' to complete netlist executions = ', Total_Executions)\n",
        "\n",
        "        for setNo in range(sets):\n",
        "            for fetNo in range(w_sets):\n",
        "                for rowNo in range(0,rows-w_rows+1,1):\n",
        "                    for colNo in range(0,cols-w_cols+1,1):\n",
        "                        temp = 0\n",
        "                        for F in range(0,w_fets,1):\n",
        "                            for R in range(w_rows):\n",
        "                                for C in range(w_cols):\n",
        "                                    temp = temp + int(not(numpy.logical_xor(bool(inputNP[setNo][F][R][C]), bool(wt[fetNo][F][R][C]))))\n",
        "                        if temp > 4:\n",
        "                            output[setNo,fetNo,rowNo,colNo] = 1\n",
        "                        else:\n",
        "                            output[setNo,fetNo,rowNo,colNo] = 0\n",
        "        output = torch.tensor(output, dtype = torch.float)\n",
        "        # output = self.conv(x)\n",
        "        return(output)\n",
        "\n",
        "\n",
        "def test_SFQ(evaluate=False):\n",
        "    global best_acc\n",
        "    model_test.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    bin_op.binarization()\n",
        "    j = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "    # for data, target in test_loader:\n",
        "        j = j + 1\n",
        "        if  j > 0:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data, volatile=True), Variable(target)\n",
        "            output = model_test(data)\n",
        "            test_loss += criterion(output, target).data.item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            # print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            #     epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            #     100. * batch_idx / len(train_loader), test_loss))\n",
        "    \n",
        "    acc = 100. * float(correct) / len(test_loader.dataset)\n",
        "    if (acc > best_acc):\n",
        "        best_acc = acc\n",
        "        # if not evaluate:\n",
        "            # save_state(model, best_acc)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        test_loss * args.batch_size, correct, len(test_loader.dataset),\n",
        "        100. * float(correct) / len(test_loader.dataset)))\n",
        "    print('Best Accuracy: {:.2f}%\\n'.format(best_acc))\n",
        "    return\n",
        "\n",
        "\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 15 epochs\"\"\"\n",
        "    lr = args.lr * (0.1 ** (epoch // args.lr_epochs))\n",
        "    print('Learning rate:', lr)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqhawAGN0ciZ",
        "colab_type": "code",
        "outputId": "1c9c4943-a49d-4495-ca45-57310ec23480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import copy\n",
        "\n",
        "weightData = copy.deepcopy(model.bin_conv2.conv.conv.weight.data)\n",
        "oneMat = torch.ones(weightData.shape)\n",
        "zeroMat = torch.zeros(weightData.shape)\n",
        "weightData_N = torch.sigmoid(weightData)\n",
        "weightData_BN = torch.where(weightData_N > 0.5, oneMat, zeroMat)\n",
        "# print(weightData_BN)\n",
        "\n",
        "model_test = copy.deepcopy(model)\n",
        "bin_conv = convSFQ_test(2, 2, kernel_size=5, stride=1, padding = 0, groups=1)\n",
        "model_test.bin_conv2.conv = bin_conv\n",
        "model_test.bin_conv2.conv.conv.weight.data = copy.deepcopy(weightData_BN.data)\n",
        "# print(model_test.bin_conv2.conv.conv.weight.data)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 2 5 1 0 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8-W2Pbf2G_g",
        "colab_type": "code",
        "outputId": "88bfbf34-4fd3-49b1-a2a5-746f6e46b96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import numpy as np\n",
        "test_SFQ()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 6.2334, Accuracy: 1135/10000 (11.35%)\n",
            "Best Accuracy: 81.70%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}